# -*- coding: utf-8 -*-
"""4.Loss Functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tIEdXm7RfKWF5JCyWOQyGL8eGR2t3DlM
"""



"""# 4-1 Datasets

## Code 4-1-1 Dataset for Regression (Linear)
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 8,5 # 5개의 데이터, 8개의 미니배치
t_weights =tf.constant([1,2,3,4,5], dtype=tf.float32)
t_bias = tf.constant([10], dtype=tf.float32)

print(t_weights)
print(t_bias)

X = tf.random.normal(mean=0, stddev=1, shape=(N,n_feature))
Y = tf.reduce_sum(t_weights * X, axis=1) + t_bias

print('X(shape,dtype,data): {} , {} \n,{}\n' . format(X.shape,X.dtype,X.numpy()))
print('Y(shape,dtype,data): {} , {} \n,{}\n' . format(Y.shape,Y.dtype,Y.numpy()))

"""## Code 4-1-2 Dataset for Binary Classification"""

import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 8,5 # 5개의 데이터, 8개의 미니배치
t_weights =tf.constant([1,2,3,4,5], dtype=tf.float32)
t_bias = tf.constant([10], dtype=tf.float32)

print(t_weights)
print(t_bias)

X = tf.random.normal(mean=0, stddev=1, shape=(N,n_feature))
Y = tf.reduce_sum(t_weights * X, axis=1) + t_bias
Y = tf.cast(Y>5 , tf.int32)


print('X(shape,dtype,data): {} , {} \n,{}\n' . format(X.shape,X.dtype,X.numpy()))
print('Y(shape,dtype,data):{} , {} \n,{}\n' . format(Y.shape,Y.dtype,Y.numpy()))

"""## Code 4-1-3 Dataset for Multi-Class lassification"""

import tensorflow as tf
import matplotlib.pyplot as plt

plt.style.use('seaborn')


N, n_feature = 8,2 
n_class = 5

X = tf.zeros(shape=(0,n_feature))
Y = tf.zeros(shape=(0,1), dtype=tf.int32)

fig, ax = plt.subplots(figsize=(5,5))
for class_idx in range(n_class) :
  center = tf.random.uniform(minval=-15,maxval=15,shape=(2,))
  #ax.scatter(center[0],center[1])
  x1 = center[0] +tf.random.normal(shape=(N,1))
  x2 = center[1]+ tf.random.normal(shape=(N,1))

  #ax.scatter(x1.numpy(),x2.numpy())

  x=tf.concat((x1,x2), axis=1)
  y= class_idx*tf.ones(shape=(N,1), dtype=tf.int32)

  ax.scatter(x[:,0].numpy(), x[:,1].numpy(), alpha=0.3)

  X= tf.concat((X,x), axis=0)
  Y = tf.concat((Y,y), axis=0)
  
  print('X(shape,dtype,data): {} , {} \n,{}\n' . format(X.shape,X.dtype,X.numpy()))
  print('Y(shape,dtype,data):{} , {} \n,{}\n' . format(Y.shape,Y.dtype,Y.numpy()))
  print(x.shape,y.shape)

"""## Code 4-1-4 Dataset for Multi-class Classification with One-hot Encoding"""

import tensorflow as tf
import matplotlib.pyplot as plt

plt.style.use('seaborn')


N, n_feature = 8,2 
n_class = 5

X = tf.zeros(shape=(0,n_feature))
Y = tf.zeros(shape=(0,), dtype=tf.int32)

fig, ax = plt.subplots(figsize=(5,5))
for class_idx in range(n_class) :
  center = tf.random.uniform(minval=-15,maxval=15,shape=(2,))
  #ax.scatter(center[0],center[1])
  x1 = center[0] +tf.random.normal(shape=(N,1))
  x2 = center[1]+ tf.random.normal(shape=(N,1))

  #ax.scatter(x1.numpy(),x2.numpy())

  x=tf.concat((x1,x2), axis=1)
  y= class_idx*tf.ones(shape=(N,), dtype=tf.int32)

  ax.scatter(x[:,0].numpy(), x[:,1].numpy(), alpha=0.3)

  X= tf.concat((X,x), axis=0)
  Y = tf.concat((Y,y), axis=0)

Y = tf.one_hot(Y, depth=n_class, dtype= tf.int32)  
print('X(shape,dtype,data): {} , {} \n,{}\n' . format(X.shape,X.dtype,X.numpy()))
print('Y(shape,dtype,data):{} , {} \n,{}\n' . format(Y.shape,Y.dtype,Y.numpy()))
print(x.shape,y.shape)

"""## Code 4-1-5 Dataset Objects"""

import tensorflow as tf
from tensorflow.keras.layers import Dense

N, n_feature = 100,5 # 5개의 데이터, 8개의 미니배치
batch_size =32 # 32씩 꺼내옴


t_weights =tf.constant([1,2,3,4,5], dtype=tf.float32)
t_bias = tf.constant([10], dtype=tf.float32)

print(t_weights)
print(t_bias)

X = tf.random.normal(mean=0, stddev=1, shape=(N,n_feature))
Y = tf.reduce_sum(t_weights * X, axis=1) + t_bias

#for batch_idx in range(N//batch_size):
#  x = X[batch_idx *batch_size : (batch_idx +1)*batch_size, ...]
#  y = Y[batch_idx *batch_size : (batch_idx +1)*batch_size, ...]
#  print(x.shape,y.shape)

dataset = tf.data.Dataset.from_tensor_slices((X,Y)) # 데이터가 적을 때
dataset = dataset.batch(batch_size).shuffle(100)


for x,y in dataset:
  print(x.shape,y.shape)

"""# 4-2 Mean Squared Error

## Code 4-2-1 MSE Calculation
"""

import tensorflow as tf

from tensorflow.keras.losses import MeanSquaredError

loss_object = MeanSquaredError()

batch_size =32
predictions = tf.random.normal(shape=(batch_size,1))
labels = tf.random.normal(shape=(batch_size,1))

mse = loss_object(labels,predictions)
mse_manual = tf.reduce_mean(tf.math.pow(labels - predictions,2))
print("MSE(tensorflow): ", mse.numpy())
print("MSE(manual): ", mse_manual.numpy())

"""## Code 4-2-2 MSE with Model/Dataset"""

import tensorflow as tf

from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import MeanSquaredError

N, n_feature = 100,5
batch_size = 32

X =tf.random.normal(shape=(N,n_feature))
Y =tf.random.normal(shape=(N,1))

dataset = tf.data.Dataset.from_tensor_slices((X,Y))
dataset = dataset.batch(batch_size)

model = Dense(units=1, activation='linear')
loss_object = MeanSquaredError()

for x,y in dataset:
  predictions = model(x)
  loss = loss_object(y,predictions)
  print(loss.numpy())

"""# 4-3 Binary Cross Entropy

## Code 4-3-1 BCE Calculation
"""

import tensorflow as tf

from tensorflow.keras.losses import BinaryCrossentropy

batch_size=32
n_class =2

predictions =tf.random.uniform(shape=(batch_size,1) ,minval=0, maxval=1,dtype=tf.float32)

labels = tf.random.uniform(shape=(batch_size,1), minval=0, maxval= n_class , dtype=tf.int32 )

loss_object = BinaryCrossentropy()
loss= loss_object(labels,predictions)

labels = tf.cast(labels,tf.float32)
bce_man = -(labels*tf.math.log(predictions) + (1-labels)*tf.math.log(1-predictions))
bce_man = tf.reduce_mean(bce_man)

print("BCE(Tensorflow): ", loss.numpy())
print("BCE(Manual): ", bce_man.numpy())

"""## Code 4-3-2 BCE with Model/Dataset"""

import tensorflow as tf

from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import BinaryCrossentropy

N, n_feature = 100,5 
t_weights =tf.constant([1,2,3,4,5], dtype=tf.float32)
t_bias = tf.constant([10], dtype=tf.float32)


X = tf.random.normal(mean=0, stddev=1, shape=(N,n_feature))
Y = tf.reduce_sum(t_weights * X, axis=1) + t_bias
Y = tf.cast(Y>5 , tf.int32)


dataset = tf.data.Dataset.from_tensor_slices((X,Y))
dataset = dataset.batch(batch_size)

model = Dense(units=1, activation='sigmoid')
loss_object=BinaryCrossentropy()

for x,y in dataset:
  prediction = model(x)
  loss = loss_object(y, predictions)
  print(loss.numpy())

"""# 4-4 Sparse Categorical Cross Entropy

## Code 4-4-1 SCCE Calculation
"""

import tensorflow as tf

from tensorflow.keras.losses import SparseCategoricalCrossentropy

batch_size , n_class= 16,5

predictions = tf.random.uniform(shape=(batch_size, n_class), minval=0, maxval=1,dtype=tf.float32)
pred_sum = tf.reshape(tf.reduce_sum(predictions, axis=1), (-1,1))

predictions= predictions/pred_sum

labels= tf.random.uniform(shape=(batch_size, ) , minval=0, maxval=n_class, dtype=tf.int32)

print(labels)

loss_object = SparseCategoricalCrossentropy()
loss = loss_object(labels, predictions)

print(loss.numpy())


ce=0

for label,prediction in zip(labels,predictions):
  ce += -tf.math.log(prediction[label])

ce/= batch_size
print(ce.numpy())

"""## Code 4-4-2 SCCE with Model/Dataset"""

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Dense
from tensorflow.keras.losses import SparseCategoricalCrossentropy


N, n_feature = 100,2 
n_class = 5

X = tf.zeros(shape=(0,n_feature))
Y = tf.zeros(shape=(0,1), dtype=tf.int32)

fig, ax = plt.subplots(figsize=(5,5))
for class_idx in range(n_class) :

  center = tf.random.uniform(minval=-15,maxval=15,shape=(2,))

  x1 = center[0] +tf.random.normal(shape=(N,1))
  x2 = center[1]+ tf.random.normal(shape=(N,1))


  x=tf.concat((x1,x2), axis=1)
  y= class_idx*tf.ones(shape=(N,1), dtype=tf.int32)


  X= tf.concat((X,x), axis=0)
  Y = tf.concat((Y,y), axis=0)
  
dataset = tf.data.Dataset.from_tensor_slices((X,Y))
dataset = dataset.batch(batch_size)


model = Dense(units=n_class, activation='softmax')
loss_object = SparseCategoricalCrossentropy()

for x,y in dataset:
  predictions = model(x)
  print(predictions.shape, y.shape)
  loss = loss_object(y,predictions)
  print(loss.numpy())

"""# 4-5 Categorical Cross Entropy

## Code 4-5-1 CCE Calculation
"""

import tensorflow as tf
from tensorflow.keras.losses import CategoricalCrossentropy

batch_size, n_class = 16,5

predictions = tf.random.uniform(shape=(batch_size, n_class), minval=0, maxval=1,dtype=tf.float32)
pred_sum = tf.reshape(tf.reduce_sum(predictions, axis=1), (-1,1))

predictions= predictions/pred_sum

labels= tf.random.uniform(shape=(batch_size, ) , minval=0, maxval=n_class, dtype=tf.int32)
labels = tf.one_hot(labels,n_class)


loss_object = CategoricalCrossentropy()
loss = loss_object(labels, predictions)

print("CCE(tensorflow): ", loss.numpy())

tmp=tf.reduce_mean(tf.reduce_sum(-labels*tf.math.log(predictions), axis =1))
print('CCE(manual): ', tmp.numpy())

"""## Code 4-5-2 CCE with Model/Dataset"""

import tensorflow as tf
from tensorflow.keras.losses import CategoricalCrossentropy

N, n_feature = 8,2 
n_class = 5

X = tf.zeros(shape=(0,n_feature))
Y = tf.zeros(shape=(0,), dtype=tf.int32)

fig, ax = plt.subplots(figsize=(5,5))
for class_idx in range(n_class) :
  center = tf.random.uniform(minval=-15,maxval=15,shape=(2,))

  x1 = center[0] +tf.random.normal(shape=(N,1))
  x2 = center[1]+ tf.random.normal(shape=(N,1))


  x=tf.concat((x1,x2), axis=1)
  y= class_idx*tf.ones(shape=(N,), dtype=tf.int32)


  X= tf.concat((X,x), axis=0)
  Y = tf.concat((Y,y), axis=0)

Y = tf.one_hot(Y, depth=n_class, dtype= tf.int32)  

dataset = tf.data.Dataset.from_tensor_slices((X,Y))
dataset = dataset.batch(batch_size)


model = Dense(units=n_class, activation='softmax')
loss_object = CategoricalCrossentropy()

for x,y in dataset:
  predictions = model(x)
  print(predictions.shape, y.shape)
  loss = loss_object(y,predictions)
  print(loss.numpy())

