{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blendlee/Deeplearning/blob/main/%5BBasic_1%5D_Data_Preprocessing_%26_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gF9XB-n8hK"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## 기본과제 1: Data Preprocessing & Tokenization\n",
        "\n",
        "> Reference 코드는 Solution 과 함께 공개됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5lPLgb7n8hO"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "* 본 과제의 목적은 자연어 처리 모델에 활용하는 텍스트 데이터를 전치리 및 토큰화 과정의 개념을 익히는 것 입니다.\n",
        "* 영어 텍스트에선 토큰화 및 Vocabulary 작성을 통해 토큰화의 기본을 배우고 [Spacy](https://spacy.io/)으로 불용어를 제외 및 전처리합니다.\n",
        "* 한국어 텍스트에선 [Konlpy](https://konlpy.org/ko/latest/)를 활용하여 형태소 기반 토큰화를 진행합니다.\n",
        "* **ANSWER HERE** 이라고 작성된 부분을 채워 완성하시면 됩니다. 다른 부분의 코드를 변경하면 오류가 발생할 수 있습니다.\n",
        "\n",
        "> 과제 완성 후 ipynb 파일을 제출해 주세요.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24GIancon8hP"
      },
      "source": [
        "### 0. 데이터 업로드\n",
        "\n",
        "1. Boostcourse [기본 과제] Data Preprocessing & Tokenization 에서 `corpus.txt` 파일을 다운받습니다.\n",
        "2. 본 Colab 환경에 `corpus.txt`, 파일을 업로드합니다.\n",
        "3. `!ls` command 를 실행했을 때, `corpus.txt sample_data` 가 나오면 성공적으로 데이터 준비가 완료된 것 입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "967zP92Fn8hQ",
        "outputId": "e06c83a0-1938-4364-ef21-0cd180a3bf53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "! ls\n",
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y8T2dDqyq9Eq",
        "outputId": "28d0aeb1-974a-4c85-d849-979826a336e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UnVdvPUWn8hR",
        "outputId": "f3aa9302-cf92-4ba8-e96f-32994040dbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1071\n",
            "A young man participates in a career while the subject who records it smiles.\n",
            "\n",
            "The man is scratching the back of his neck while looking for a book in a book store.\n",
            "\n",
            "A person wearing goggles and a hat is sled riding.\n",
            "\n",
            "A girl in a pink coat and flowered goloshes sledding down a hill.\n",
            "\n",
            "Three girls are standing in front of a window of a building.\n",
            "\n",
            "two dog is playing with a same chump on their mouth\n",
            "\n",
            "Low angle view of people suspended from the swings of a carnival ride.\n",
            "\n",
            "Black and white photo of two people watching a beautiful painting\n",
            "\n",
            "Man dressed in black crosses the street screaming\n",
            "\n",
            "A child sits on street on a busy street.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Week8_NLP/corpus.txt', 'r', encoding='utf-8') as fd:\n",
        "    corpus = fd.readlines()\n",
        "\n",
        "# 말뭉치 크기 확인\n",
        "print(len(corpus))\n",
        "\n",
        "# 첫 열 문장을 print 해 봅시다.\n",
        "for sentence in corpus[:10]:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_x6hvnn8hS"
      },
      "source": [
        "### 1. 파이썬 기본 코드를 이용한 영어 텍스트 토큰화 및 전처리\n",
        "\n",
        "\n",
        "💡 토큰화(tokenization)는 무엇인가요?\n",
        "\n",
        "토큰화는 주어진 입력 데이터를 자연어처리 모델이 인식할 수 있는 단위로 변환해주는 방법입니다. \n",
        "\n",
        "💡 단어 단위 토큰화(word tokenization)는요?\n",
        "\n",
        "단어단위 토큰화의 경우 \"단어\"가 자연어처리 모델이 인식하는 단위가 됩니다.\n",
        "\"I have a meal\"이라고 하는 문장을 가지고 단어 단위 토큰화를 하면 다음과 같습니다. \n",
        "\n",
        "- ['I', 'have', 'a', 'meal']\n",
        "\n",
        "영어의 경우 대부분 공백(space)을 기준으로 단어가 정의되기 때문에 `.split()`을 이용해 쉽게 단어 단위 토큰화를 구현할 수 있습니다.\n",
        "특히, 영어에서 공백을 기준으로 단어를 구분한 단어 단위 토큰화는 공백 단위 토큰화 (space tokenization)이라고도 할 수 있습니다.\n",
        "\n",
        "> 이 섹션에서는 파이썬 표준 라이브러리 (Python Standard Libary)만을 사용하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQjQ7Oyn8hT"
      },
      "source": [
        "#### 1-A) 토큰화기 (tokenizer) 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "64BETuP8n8hU"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import re\n",
        "def tokenize(\n",
        "    sentence: str\n",
        ") -> List[str]:\n",
        "    \"\"\" 토큰화기 구현\n",
        "    공백으로 토큰을 구분하되 . , ! ? 문장 부호는 별개의 토큰으로 처리되어야 합니다.\n",
        "    영문에서 Apostrophe에 해당하는 ' 는 두가지 경우에 대해 처리해야합니다.\n",
        "    1. not의 준말인 n't은 하나의 토큰으로 처리되어야 합니다: don't ==> do n't\n",
        "    2. 다른 Apostrophe 용법은 뒤의 글자들을 붙여서 처리합니다: 's 'm 're 등등 \n",
        "    그 외 다른 문장 부호는 고려하지 않으며, 작은 따옴표는 모두 Apostrophe로 처리합니다.\n",
        "    모든 토큰은 소문자로 변환되어야 합나다.\n",
        "\n",
        "    힌트: 정규표현식을 안다면 re 라이브러리를 사용해 보세요!\n",
        "\n",
        "    예시: 'I don't like Jenifer's work.'\n",
        "    ==> ['i', 'do', 'n\\'t', 'like', 'jenifer', '\\'s', 'work', '.']\n",
        "\n",
        "    Arguments:\n",
        "    sentence -- 토큰화할 영문 문장\n",
        "    \n",
        "    Return:\n",
        "    tokens -- 토큰화된 토큰 리스트\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    sentence=sentence.lower()\n",
        "\n",
        "    \n",
        "    sentence = re.split('(\\'[a-z]+|n\\'t|\\W)',sentence)\n",
        "    \n",
        "\n",
        "    \n",
        "    sentence = list(filter(lambda x: x != ' ' and  x != '', sentence))\n",
        "    tokens: List[str] = list()\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNFuA7rn8hV"
      },
      "source": [
        "**문제 1-A에 대한 테스트 코드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "sGKBZTWAn8hX",
        "outputId": "67b145a7-b0a9-4afa-c369-da8f2ccd65bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Tokenizer Test Cases======\n",
            "첫번째 테스트 통과!\n",
            "두번째 테스트 통과!\n",
            "모든 테스트 통과!\n"
          ]
        }
      ],
      "source": [
        "print (\"======Tokenizer Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentence = \"This sentence should be tokenized properly.\"\n",
        "\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "assert tokens == ['this', 'sentence', 'should', 'be', 'tokenized', 'properly', '.'], \\\n",
        "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
        "print(\"첫번째 테스트 통과!\")\n",
        "\n",
        "# Second test\n",
        "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "assert tokens == [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"], \\\n",
        "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
        "print(\"두번째 테스트 통과!\")\n",
        "\n",
        "print(\"모든 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWT1LKWn8hY"
      },
      "source": [
        "### 1-B) Vocabulary 만들기\n",
        "컴퓨터는 글자를 알아볼 수 없기 때문에 각 토큰을 숫자 형식의 유일한 id에 매핑해야합니다.\n",
        "\n",
        "- ['I', 'have', 'a', 'meal'] ==> [194, 123, 2, 54]\n",
        "\n",
        "이러한 매핑은 모델 학습 전에 사전 정의되어야합니다.\n",
        "이때, 모델이 다를 수 있는 토큰들의 집합과 이 매핑을 Vocab라고 흔히 부릅니다.\n",
        "\n",
        "이 매핑을 만들어 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "WukE7ytQn8hY"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "from collections import Counter\n",
        "# [UNK] 토큰\n",
        "unk_token = \"[UNK]\"\n",
        "unk_token_id = 0 # [UNK] 토큰의 id는 0으로 처리합니다.\n",
        "\n",
        "def build_vocab(\n",
        "    sentences: List[List[str]],\n",
        "    min_freq: int\n",
        ") -> Tuple[List[str], Dict[str, int], List[int]]:\n",
        "    \"\"\" Vocabulary 만들기\n",
        "    토큰화된 문장들을 받아 각 토큰을 숫자로 매핑하는 token2id와 그 역매핑인 id2token를 만듭니다.\n",
        "    자주 안나오는 단어는 과적합을 일으킬 수 있기 때문에 빈도가 적은 단어는 [UNK] 토큰으로 처리합니다.\n",
        "    이는 Unknown의 준말입니다.\n",
        "    토큰의 id 번호 순서는 [UNK] 토큰을 제외하고는 자유입니다.\n",
        "\n",
        "    힌트: collection 라이브러리의 Counter 객체를 활용해보세요.\n",
        "\n",
        "    Arguments:\n",
        "    sentences -- Vocabulary를 만들기 위한 토큰화된 문장들\n",
        "    min_freq -- 단일 토큰으로 처리되기 위한 최소 빈도\n",
        "                데이터셋에서 최소 빈도보다 더 적게 등장하는 토큰은 [UNK] 토큰으로 처리되어야 합니다.\n",
        "\n",
        "    Return:\n",
        "    id2token -- id를 받으면 해당하는 토큰을 반환하는 리스트 \n",
        "    token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    ### ANSWER HERE ###\n",
        "    tokens=[]\n",
        "    for sentence in sentences:\n",
        "      tokens+=sentence\n",
        "    \n",
        "\n",
        "    counter= dict(Counter(tokens))\n",
        "    token2id: Dict[str, int] = {unk_token: unk_token_id}\n",
        "\n",
        "    id2token: List[str] = [unk_token]\n",
        "    for key,values in counter.items():\n",
        "      if values < min_freq:\n",
        "        continue\n",
        "      id2token.append(key)\n",
        "      token2id[key] = id2token.index(key)\n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    assert id2token[unk_token_id] == unk_token and token2id[unk_token] == unk_token_id, \\\n",
        "        \"[UNK] 토큰을 적절히 삽입하세요\"\n",
        "    assert len(id2token) == len(token2id), \\\n",
        "        \"id2word과 word2id의 크기는 같아야 합니다\"\n",
        "    return id2token, token2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM-682sin8hZ"
      },
      "source": [
        "**문제 1-B에 대한 테스트 코드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "rE9cjJWgn8hZ",
        "outputId": "3942971b-9041-4b93-a263-53967b0be9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Vocabulary Builder Test Cases======\n",
            "첫번째 테스트 통과!\n",
            "두번째 테스트 통과!\n",
            "모든 테스트 통과!\n"
          ]
        }
      ],
      "source": [
        "print (\"======Vocabulary Builder Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentences = [[\"this\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"],\n",
        "                [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]]\n",
        "\n",
        "id2token, token2id = build_vocab(sentences, min_freq=1)\n",
        "assert sentences == [[id2token[token2id[token]] for token in sentence] for sentence in sentences], \\\n",
        "    \"token2id와 id2token이 서로 역매핑이 아닙니다.\"\n",
        "print(\"첫번째 테스트 통과!\")\n",
        "\n",
        "# Second test\n",
        "sentences = [[\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
        "                [\"c\", \"d\", \"f\", \"g\"],\n",
        "                [\"d\", \"e\", \"g\", \"h\"]]\n",
        "\n",
        "id2token, token2id = build_vocab(sentences, min_freq=2)\n",
        "assert set(token2id.keys()) == {unk_token, 'c', 'd', 'e', 'g'} == set(id2token) and len(id2token) == 5, \\\n",
        "    \"min_freq 인자가 제대로 작동되지 않습니다.\"\n",
        "print(\"두번째 테스트 통과!\")\n",
        "\n",
        "print(\"모든 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tvfob9_n8ha"
      },
      "source": [
        "### 1-C) 인코딩 및 디코딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimftJ_Wn8ha"
      },
      "source": [
        "이제 문장을 받아 토큰화하고 이들을 적절한 id들로 바꾸는 인코딩 함수를 작성해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "SkeX30vJn8ha"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def encode(\n",
        "    tokenize: Callable[[str], List[str]],\n",
        "    sentence: str,\n",
        "    token2id: Dict[str, int]\n",
        ") -> List[str]:\n",
        "    \"\"\" 인코딩\n",
        "    문장을 받아 토큰화하고 이들을 적절한 id들로 바꿉니다.\n",
        "    토큰화 및 인덱싱은 인자로 들어온 tokenize 함수와 인자로 주어진 token2id를 활용합니다.\n",
        "    Vocab에 없는 단어는 [UNK] 토큰으로 처리합니다.\n",
        "\n",
        "    Arguments:\n",
        "    tokenize -- 토큰화 함수: 문장을 받으면 토큰들의 리스트를 반환하는 함수\n",
        "    sentence -- 토큰화할 영문 문장\n",
        "    token2id -- 토큰을 받으면 해당하는 id를 반환하는 딕셔너리\n",
        "    \n",
        "    Return:\n",
        "    token_ids -- 문장을 인코딩하여 숫자로 변환한 리스트\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    token_ids: List[int] = list()\n",
        "\n",
        "    tokens=tokenize(sentence)\n",
        "    for token in tokens:\n",
        "      if token in token2id:\n",
        "        token_ids.append(token2id[token])\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return token_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2WVOPIln8hb"
      },
      "source": [
        "거꾸로 id들이 있을 때 원문장을 복원하는 디코딩 함수도 필요합니다.\n",
        "그러나 토큰화 과정에서 공백 및 대소문자 정보를 잃어버리고, [UNK] 토큰으로 인해 원문장을 복원할 수 없습니다.\n",
        "때문에, 단순히 공백으로 연결된 문장으로 디코딩합시다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "0Rl1a-v9n8hb"
      },
      "outputs": [],
      "source": [
        "def decode(\n",
        "    token_ids: List[int],\n",
        "    id2token: List[str]\n",
        ") -> str:\n",
        "    \"\"\" 디코딩\n",
        "    각 id를 적절한 토큰으로 바꾸고 공백으로 연결하여 문장을 반환합니다.\n",
        "    \"\"\"\n",
        "    return ' '.join(id2token[token_id] for token_id in token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_WWwt1yn8hb"
      },
      "source": [
        "**앞서 만든 함수로 말뭉치를 인코딩해봅시다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "bpN8gyv9n8he"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "id2token, token2id = build_vocab(list(map(tokenize, corpus)), min_freq=2)\n",
        "input_ids = list(map(partial(encode, tokenize, token2id=token2id), corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "f_d1_m2Xn8hf",
        "outputId": "fd467c15-3906-4ac0-d5f7-26607439a7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======1=====\n",
            "원문: A young man participates in a career while the subject who records it smiles.\n",
            "\n",
            "인코딩 결과: [1, 2, 3, 4, 1, 5, 6, 7, 8, 9, 10]\n",
            "디코딩 결과: a young man in a while the who it . \n",
            "\n",
            "\n",
            "======2=====\n",
            "원문: The man is scratching the back of his neck while looking for a book in a book store.\n",
            "\n",
            "인코딩 결과: [6, 3, 11, 12, 6, 13, 14, 15, 5, 16, 17, 1, 18, 4, 1, 18, 19, 9, 10]\n",
            "디코딩 결과: the man is scratching the back of his while looking for a book in a book store . \n",
            "\n",
            "\n",
            "======3=====\n",
            "원문: A person wearing goggles and a hat is sled riding.\n",
            "\n",
            "인코딩 결과: [1, 20, 21, 22, 23, 1, 24, 11, 25, 9, 10]\n",
            "디코딩 결과: a person wearing goggles and a hat is riding . \n",
            "\n",
            "\n",
            "======4=====\n",
            "원문: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
            "\n",
            "인코딩 결과: [1, 26, 4, 1, 27, 28, 23, 29, 30, 1, 31, 9, 10]\n",
            "디코딩 결과: a girl in a pink coat and flowered down a hill . \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
        "    print(f\"======{sid}=====\")\n",
        "    print(f\"원문: {sentence}\")\n",
        "    print(f\"인코딩 결과: {token_ids}\"),\n",
        "    print(f\"디코딩 결과: {decode(token_ids, id2token)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2Mq2_on8hf"
      },
      "source": [
        "### 2. [Spacy](https://spacy.io/)를 이용한 영어 텍스트 토큰화 및 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "olkGCTT1n8hf",
        "outputId": "a1e353ba-019d-4913-efb2-1abf3419cad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xvivhdbn8hf"
      },
      "source": [
        "### 2-A) Spacy를 활용법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "CHnlmfnqn8hg"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy_tokenizer = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlKPMMrtn8hg"
      },
      "source": [
        "Spacy를 활용한 토큰화는 상대적으로 시간이 오래 걸리지만, 토큰화 외에도 품사 및 단어의 기본형 정보 등 해당 문장에 대해 많은 정보를 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "2QYwB3Bcn8hg",
        "outputId": "79a89eca-be61-4575-dde1-144f74a325da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Jhon', 'Jhon', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('book', 'book', 'NOUN'), ('is', 'be', 'AUX'), (\"n't\", 'not', 'PART'), ('popular', 'popular', 'ADJ'), (',', ',', 'PUNCT'), ('but', 'but', 'CCONJ'), ('he', '-PRON-', 'PRON'), ('loves', 'love', 'VERB'), ('his', '-PRON-', 'DET'), ('book', 'book', 'NOUN'), ('.', '.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "tokens = spacy_tokenizer(\"Jhon's book isn't popular, but he loves his book.\")\n",
        "print ([(token.text, token.lemma_, token.pos_) for token in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaHwA3Lsn8hg"
      },
      "source": [
        "**불용어(Stopword)**\n",
        "\n",
        "불용어란 한 언어에서 자주 등장하지만 큰 의미가 없는 단어를 뜯합니다.\n",
        "고전적인 자연어 처리에서는 이러한 단어들은 분석에 도움이 되지 않는다고 생각하였기 때문에 이를 제거합니다.\n",
        "`Spacy`에서는 불용어 단어의 목록을 제공하고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "kY5QqCTBn8hg",
        "outputId": "95bdd020-fe25-484f-ff10-90b98a1d6b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fifty', \"'d\", 'latter', 'twelve', 'up', 'him', 'whoever', 'mine', 'one', 'may', 'sometimes', 'next', 'who', 'former', 'sometime', 'while', '‘m', 'very', 'take', 'put', 'their', 'whom', 'twenty', 'go', 'someone', 'and', 'alone', 'became', 'over', 'was', \"'m\", 'rather', 'they', 'her', 'your', '’m', 'across', 'why', 'nevertheless', 'using', 're', 'formerly', 'top', 'wherever', 'them', 'myself', 'enough', 'us', 'whose', 'hundred', 'without', 'many', 'ours', 'can', 'even', 'themselves', 'thereupon', 'whether', 'noone', 'being', 'fifteen', 'could', 'four', 'under', 'eight', 'has', 'as', 'therein', 'yet', 'last', 'because', 'these', '’ve', 'upon', 'how', 'thus', 'meanwhile', 'nine', 'though', 'on', '‘re', 'yourselves', 'whence', 'therefore', 'neither', 'always', 'else', 'less', \"'re\", 'beforehand', 'say', '’d', 'bottom', 'so', 'its', 'least', 'wherein', 'anything', 'throughout', 'such', 'per', 'each', 'perhaps', 'before', 'also', 'show', 'becoming', 'name', '’s', 'am', 'what', 'within', 'does', 'onto', 'with', 'n’t', 'another', 'both', 'my', 'either', 'the', 'would', 'were', 'made', '‘ll', 'everything', 'or', 'beside', 'down', 'should', 'then', 'sixty', 'hereupon', \"'ll\", 'along', 'from', 'together', 'nobody', 'thereafter', 'now', 'back', '‘ve', 'here', 'other', 'nothing', 'have', 'too', 'used', 'off', 'otherwise', 'some', 'seeming', 'further', 'nowhere', 'all', 'a', 'anyone', 'whither', 'only', 'nor', 'others', 'thence', 'after', 'his', 'still', 'really', 'itself', 'that', 'never', 'yourself', 'whole', 'every', 'herein', 'whereafter', 'more', 'doing', 'often', 'everywhere', 'of', 'hers', 'below', 'above', 'me', 'done', 'than', 'toward', 'give', 'somehow', 'besides', 'i', 'not', 'but', 'where', 'moreover', 'two', 'unless', 'most', 'seems', 'five', 'something', 'almost', 'mostly', 'already', 'anyhow', 'regarding', 'until', 'serious', 'might', 'quite', '’ll', 'cannot', 'through', 'third', 'whereby', 'towards', 'eleven', 'same', 'between', 'hence', 'no', 'first', 'if', 'much', 'did', '’re', 'seemed', 'at', 'whenever', 'somewhere', 'none', 'be', 'when', 'elsewhere', 'due', 'again', 'well', 'via', 'please', 'forty', 'this', 'although', 'three', 'except', 'own', 'himself', 'n‘t', 'move', 'will', 'among', 'are', 'just', 'into', 'against', 'had', 'part', 'is', 'full', 'afterwards', 'keep', 'indeed', 'anyway', 'once', 'there', \"'ve\", 'becomes', 'she', 'get', 'however', 'since', 'in', 'we', 'he', 'been', 'beyond', 'amongst', 'ourselves', 'whereupon', 'anywhere', 'front', 'ten', 'various', 'several', 'thereby', 'amount', '‘d', 'ca', \"n't\", 'an', 'seem', 'everyone', 'hereafter', 'side', 'for', 'whereas', 'those', 'hereby', 'yours', 'about', 'by', '‘s', 'you', 'it', 'latterly', 'out', 'must', 'herself', 'ever', 'our', 'around', 'any', 'see', 'become', 'make', 'whatever', 'six', 'few', 'thru', 'do', 'call', 'behind', 'to', 'during', 'which', \"'s\", 'empty', 'namely'}\n"
          ]
        }
      ],
      "source": [
        "print(spacy.lang.en.stop_words.STOP_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeAi-qnrn8hh"
      },
      "source": [
        "### 2-B) Spacy를 활용한 전처리 및 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "aK4APB7xn8hh"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenize(\n",
        "    tokenizer: spacy.language.Language,\n",
        "    sentence: str\n",
        ") -> List[str]:\n",
        "    \"\"\" Spacy를 활용한 토크나이저 구현\n",
        "    Spacy를 활용해서 토큰화를 진행합니다. 이때 불용어는 제외하고 어간을 토큰으로 사용합니다.\n",
        "    \n",
        "    예시: 'I don't like Jenifer's work.'\n",
        "    ==> ['I', 'like', 'Jenifer', 'work', '.']\n",
        "\n",
        "    Arguments:\n",
        "    tokenizer -- Spacy 토큰화기\n",
        "    sentence -- 토큰화할 영문 문장\n",
        "    \n",
        "    Return:\n",
        "    tokens -- 불용어 제거 및 토큰화된 토큰 리스트\n",
        "    \"\"\"\n",
        "    \n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    tokens: List[str] = list()\n",
        "    sentence=sentence.lower()\n",
        "    tokenized = tokenizer(sentence)\n",
        "\n",
        "    for token in tokenized:\n",
        "      if token.text not in spacy.lang.en.stop_words.STOP_WORDS:\n",
        "        tokens.append(token.text)\n",
        "    print(tokens)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0HoPbfAn8hh"
      },
      "source": [
        "**문제 2-B에 대한 테스트 코드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "Yrimnj3qn8hh",
        "outputId": "f94d2d8c-978a-4922-811d-f150022832fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Spacy Tokenizer Test Cases======\n",
            "['jhon', 'book', 'popular', ',', 'loves', 'book', '.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-200-eab03c85cef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tokenize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'properly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;34m\"토큰화된 리스트가 기대 결과와 다릅니다.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"첫번째 테스트 통과!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 토큰화된 리스트가 기대 결과와 다릅니다."
          ]
        }
      ],
      "source": [
        "print (\"======Spacy Tokenizer Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentence = \"This sentence should be tokenized properly.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
        "assert tokens == ['this', 'sentence', 'tokenize', 'properly', '.'], \\\n",
        "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
        "print(\"첫번째 테스트 통과!\")\n",
        "\n",
        "# Second test\n",
        "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
        "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
        "assert tokens == ['Jhon', 'book', 'popular', ',', 'love', 'book', '.'], \\\n",
        "    \"토큰화된 리스트가 기대 결과와 다릅니다.\"\n",
        "print(\"두번째 테스트 통과!\")\n",
        "\n",
        "print(\"모든 테스트 통과!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2EB-FwGn8hh"
      },
      "source": [
        "**앞서 만든 함수로 말뭉치를 인코딩해봅시다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAlMF-i_n8hi"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "my_tokenize = partial(spacy_tokenize, spacy_tokenizer)\n",
        "id2token, token2id = build_vocab(list(map(my_tokenize, tqdm(corpus, desc=\"Building\"))), min_freq=3)\n",
        "input_ids = list(map(partial(encode, my_tokenize, token2id=token2id), tqdm(corpus, desc=\"Tokenizing\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUrSVg0-n8hi"
      },
      "outputs": [],
      "source": [
        "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
        "    print(f\"======{sid}=====\")\n",
        "    print(f\"원문: {sentence}\")\n",
        "    print(f\"인코딩 결과: {token_ids}\"),\n",
        "    print(f\"디코딩 결과: {decode(token_ids, id2token)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvPOazVbn8hj"
      },
      "source": [
        "### 3. [Konlpy](https://konlpy.org/ko/latest/)를 활용한 한국어 토큰화\n",
        "한국어에서 \"나는 밥을 먹는다\"라는 문장을 단어 단위 토큰화하면 다음과 같습니다.\n",
        "- ['나', '는', '밥', '을', '먹는다']\n",
        "\n",
        "한국어에서 \"단어\"는 공백을 기준으로 정의되지 않습니다. 이는 한국어가 갖고 있는 \"교착어\"로서의 특징 때문입니다. \n",
        "체언 뒤에 조사가 붙는 것이 대표적인 특징이며 의미 단위가 구분되고 자립성이 있기 때문에 조사는 \"단어\"입니다.\n",
        "\n",
        "한국어에서는 단어 단위 토큰화 방법는 공백에 기반하지 않고 사용하지 않고 형태소 분석기를 활용하고 있습니다.\n",
        "\n",
        "\n",
        "(참고 1: [국립 국어원: \"조사는 단어이다\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n",
        "\n",
        "(참고 2: [Konlpy: 형태소 분석기](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Qc2By1n8hj"
      },
      "outputs": [],
      "source": [
        "! apt-get install -y build-essential openjdk-8-jdk python3-dev curl git automake\n",
        "! pip install konlpy \"tweepy<4.0.0\"\n",
        "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjXTspTn8hj"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yNUkJRWn8hk"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\\\n",
        "유구한 역사와 전통에 빛나는 우리 대한국민은 \\\n",
        "3·1운동으로 건립된 대한민국임시정부의 법통과 불의에 항거한 4·19민주이념을 계승하고, \\\n",
        "조국의 민주개혁과 평화적 통일의 사명에 입각하여 정의·인도와 동포애로써 민족의 단결을 공고히 하고, \\\n",
        "모든 사회적 폐습과 불의를 타파하며, \\\n",
        "자율과 조화를 바탕으로 자유민주적 기본질서를 더욱 확고히 하여 \\\n",
        "정치·경제·사회·문화의 모든 영역에 있어서 각인의 기회를 균등히 하고, \\\n",
        "능력을 최고도로 발휘하게 하며, 자유와 권리에 따르는 책임과 의무를 완수하게 하여, \\\n",
        "안으로는 국민생활의 균등한 향상을 기하고 밖으로는 항구적인 세계평화와 인류공영에 이바지함으로써 \\\n",
        "우리들과 우리들의 자손의 안전과 자유와 행복을 영원히 확보할 것을 다짐하면서 \\\n",
        "1948년 7월 12일에 제정되고 8차에 걸쳐 개정된 헌법을 이제 국회의 의결을 거쳐 국민투표에 의하여 개정한다.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPf54Fuen8hk"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.pos(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mhdo9Ktn8hk"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.morphs(text))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d56be8d41f6ed650baad816e622386c6d957316b9dcbfdf7cffae878b7a3098b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('default')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "[Basic 1] Data Preprocessing & Tokenization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}