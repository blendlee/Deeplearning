{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blendlee/Deeplearning/blob/main/%5BBasic_1%5D_Data_Preprocessing_%26_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1gF9XB-n8hK"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## ê¸°ë³¸ê³¼ì œ 1: Data Preprocessing & Tokenization\n",
        "\n",
        "> Reference ì½”ë“œëŠ” Solution ê³¼ í•¨ê»˜ ê³µê°œë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5lPLgb7n8hO"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "* ë³¸ ê³¼ì œì˜ ëª©ì ì€ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì— í™œìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì¹˜ë¦¬ ë° í† í°í™” ê³¼ì •ì˜ ê°œë…ì„ ìµíˆëŠ” ê²ƒ ì…ë‹ˆë‹¤.\n",
        "* ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„  í† í°í™” ë° Vocabulary ì‘ì„±ì„ í†µí•´ í† í°í™”ì˜ ê¸°ë³¸ì„ ë°°ìš°ê³  [Spacy](https://spacy.io/)ìœ¼ë¡œ ë¶ˆìš©ì–´ë¥¼ ì œì™¸ ë° ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "* í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„  [Konlpy](https://konlpy.org/ko/latest/)ë¥¼ í™œìš©í•˜ì—¬ í˜•íƒœì†Œ ê¸°ë°˜ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "* **ANSWER HERE** ì´ë¼ê³  ì‘ì„±ëœ ë¶€ë¶„ì„ ì±„ì›Œ ì™„ì„±í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ë³€ê²½í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ê³¼ì œ ì™„ì„± í›„ ipynb íŒŒì¼ì„ ì œì¶œí•´ ì£¼ì„¸ìš”.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24GIancon8hP"
      },
      "source": [
        "### 0. ë°ì´í„° ì—…ë¡œë“œ\n",
        "\n",
        "1. Boostcourse [ê¸°ë³¸ ê³¼ì œ] Data Preprocessing & Tokenization ì—ì„œ `corpus.txt` íŒŒì¼ì„ ë‹¤ìš´ë°›ìŠµë‹ˆë‹¤.\n",
        "2. ë³¸ Colab í™˜ê²½ì— `corpus.txt`, íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "3. `!ls` command ë¥¼ ì‹¤í–‰í–ˆì„ ë•Œ, `corpus.txt sample_data` ê°€ ë‚˜ì˜¤ë©´ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œëœ ê²ƒ ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "967zP92Fn8hQ",
        "outputId": "e06c83a0-1938-4364-ef21-0cd180a3bf53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "! ls\n",
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y8T2dDqyq9Eq",
        "outputId": "28d0aeb1-974a-4c85-d849-979826a336e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UnVdvPUWn8hR",
        "outputId": "f3aa9302-cf92-4ba8-e96f-32994040dbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1071\n",
            "A young man participates in a career while the subject who records it smiles.\n",
            "\n",
            "The man is scratching the back of his neck while looking for a book in a book store.\n",
            "\n",
            "A person wearing goggles and a hat is sled riding.\n",
            "\n",
            "A girl in a pink coat and flowered goloshes sledding down a hill.\n",
            "\n",
            "Three girls are standing in front of a window of a building.\n",
            "\n",
            "two dog is playing with a same chump on their mouth\n",
            "\n",
            "Low angle view of people suspended from the swings of a carnival ride.\n",
            "\n",
            "Black and white photo of two people watching a beautiful painting\n",
            "\n",
            "Man dressed in black crosses the street screaming\n",
            "\n",
            "A child sits on street on a busy street.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('/content/drive/MyDrive/Week8_NLP/corpus.txt', 'r', encoding='utf-8') as fd:\n",
        "    corpus = fd.readlines()\n",
        "\n",
        "# ë§ë­‰ì¹˜ í¬ê¸° í™•ì¸\n",
        "print(len(corpus))\n",
        "\n",
        "# ì²« ì—´ ë¬¸ì¥ì„ print í•´ ë´…ì‹œë‹¤.\n",
        "for sentence in corpus[:10]:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY_x6hvnn8hS"
      },
      "source": [
        "### 1. íŒŒì´ì¬ ê¸°ë³¸ ì½”ë“œë¥¼ ì´ìš©í•œ ì˜ì–´ í…ìŠ¤íŠ¸ í† í°í™” ë° ì „ì²˜ë¦¬\n",
        "\n",
        "\n",
        "ğŸ’¡ í† í°í™”(tokenization)ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
        "\n",
        "í† í°í™”ëŠ” ì£¼ì–´ì§„ ì…ë ¥ ë°ì´í„°ë¥¼ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë‹¨ìœ„ë¡œ ë³€í™˜í•´ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \n",
        "\n",
        "ğŸ’¡ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”(word tokenization)ëŠ”ìš”?\n",
        "\n",
        "ë‹¨ì–´ë‹¨ìœ„ í† í°í™”ì˜ ê²½ìš° \"ë‹¨ì–´\"ê°€ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸ì´ ì¸ì‹í•˜ëŠ” ë‹¨ìœ„ê°€ ë©ë‹ˆë‹¤.\n",
        "\"I have a meal\"ì´ë¼ê³  í•˜ëŠ” ë¬¸ì¥ì„ ê°€ì§€ê³  ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ë¥¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
        "\n",
        "- ['I', 'have', 'a', 'meal']\n",
        "\n",
        "ì˜ì–´ì˜ ê²½ìš° ëŒ€ë¶€ë¶„ ê³µë°±(space)ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ê°€ ì •ì˜ë˜ê¸° ë•Œë¬¸ì— `.split()`ì„ ì´ìš©í•´ ì‰½ê²Œ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "íŠ¹íˆ, ì˜ì–´ì—ì„œ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•œ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”ëŠ” ê³µë°± ë‹¨ìœ„ í† í°í™” (space tokenization)ì´ë¼ê³ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ì´ ì„¹ì…˜ì—ì„œëŠ” íŒŒì´ì¬ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ (Python Standard Libary)ë§Œì„ ì‚¬ìš©í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FQjQ7Oyn8hT"
      },
      "source": [
        "#### 1-A) í† í°í™”ê¸° (tokenizer) êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "64BETuP8n8hU"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import re\n",
        "def tokenize(\n",
        "    sentence: str\n",
        ") -> List[str]:\n",
        "    \"\"\" í† í°í™”ê¸° êµ¬í˜„\n",
        "    ê³µë°±ìœ¼ë¡œ í† í°ì„ êµ¬ë¶„í•˜ë˜ . , ! ? ë¬¸ì¥ ë¶€í˜¸ëŠ” ë³„ê°œì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    ì˜ë¬¸ì—ì„œ Apostropheì— í•´ë‹¹í•˜ëŠ” ' ëŠ” ë‘ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ ì²˜ë¦¬í•´ì•¼í•©ë‹ˆë‹¤.\n",
        "    1. notì˜ ì¤€ë§ì¸ n'tì€ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: don't ==> do n't\n",
        "    2. ë‹¤ë¥¸ Apostrophe ìš©ë²•ì€ ë’¤ì˜ ê¸€ìë“¤ì„ ë¶™ì—¬ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤: 's 'm 're ë“±ë“± \n",
        "    ê·¸ ì™¸ ë‹¤ë¥¸ ë¬¸ì¥ ë¶€í˜¸ëŠ” ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©°, ì‘ì€ ë”°ì˜´í‘œëŠ” ëª¨ë‘ Apostropheë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "    ëª¨ë“  í† í°ì€ ì†Œë¬¸ìë¡œ ë³€í™˜ë˜ì–´ì•¼ í•©ë‚˜ë‹¤.\n",
        "\n",
        "    íŒíŠ¸: ì •ê·œí‘œí˜„ì‹ì„ ì•ˆë‹¤ë©´ re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ì„¸ìš”!\n",
        "\n",
        "    ì˜ˆì‹œ: 'I don't like Jenifer's work.'\n",
        "    ==> ['i', 'do', 'n\\'t', 'like', 'jenifer', '\\'s', 'work', '.']\n",
        "\n",
        "    Arguments:\n",
        "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
        "    \n",
        "    Return:\n",
        "    tokens -- í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    sentence=sentence.lower()\n",
        "\n",
        "    \n",
        "    sentence = re.split('(\\'[a-z]+|n\\'t|\\W)',sentence)\n",
        "    \n",
        "\n",
        "    \n",
        "    sentence = list(filter(lambda x: x != ' ' and  x != '', sentence))\n",
        "    tokens: List[str] = list()\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhNFuA7rn8hV"
      },
      "source": [
        "**ë¬¸ì œ 1-Aì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "sGKBZTWAn8hX",
        "outputId": "67b145a7-b0a9-4afa-c369-da8f2ccd65bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Tokenizer Test Cases======\n",
            "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
            "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
            "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
          ]
        }
      ],
      "source": [
        "print (\"======Tokenizer Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentence = \"This sentence should be tokenized properly.\"\n",
        "\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "assert tokens == ['this', 'sentence', 'should', 'be', 'tokenized', 'properly', '.'], \\\n",
        "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
        "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "# Second test\n",
        "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "assert tokens == [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"], \\\n",
        "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
        "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQWT1LKWn8hY"
      },
      "source": [
        "### 1-B) Vocabulary ë§Œë“¤ê¸°\n",
        "ì»´í“¨í„°ëŠ” ê¸€ìë¥¼ ì•Œì•„ë³¼ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ê° í† í°ì„ ìˆ«ì í˜•ì‹ì˜ ìœ ì¼í•œ idì— ë§¤í•‘í•´ì•¼í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ['I', 'have', 'a', 'meal'] ==> [194, 123, 2, 54]\n",
        "\n",
        "ì´ëŸ¬í•œ ë§¤í•‘ì€ ëª¨ë¸ í•™ìŠµ ì „ì— ì‚¬ì „ ì •ì˜ë˜ì–´ì•¼í•©ë‹ˆë‹¤.\n",
        "ì´ë•Œ, ëª¨ë¸ì´ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ” í† í°ë“¤ì˜ ì§‘í•©ê³¼ ì´ ë§¤í•‘ì„ Vocabë¼ê³  í”íˆ ë¶€ë¦…ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë§¤í•‘ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "WukE7ytQn8hY"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "from collections import Counter\n",
        "# [UNK] í† í°\n",
        "unk_token = \"[UNK]\"\n",
        "unk_token_id = 0 # [UNK] í† í°ì˜ idëŠ” 0ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "def build_vocab(\n",
        "    sentences: List[List[str]],\n",
        "    min_freq: int\n",
        ") -> Tuple[List[str], Dict[str, int], List[int]]:\n",
        "    \"\"\" Vocabulary ë§Œë“¤ê¸°\n",
        "    í† í°í™”ëœ ë¬¸ì¥ë“¤ì„ ë°›ì•„ ê° í† í°ì„ ìˆ«ìë¡œ ë§¤í•‘í•˜ëŠ” token2idì™€ ê·¸ ì—­ë§¤í•‘ì¸ id2tokenë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "    ìì£¼ ì•ˆë‚˜ì˜¤ëŠ” ë‹¨ì–´ëŠ” ê³¼ì í•©ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ëŠ” [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "    ì´ëŠ” Unknownì˜ ì¤€ë§ì…ë‹ˆë‹¤.\n",
        "    í† í°ì˜ id ë²ˆí˜¸ ìˆœì„œëŠ” [UNK] í† í°ì„ ì œì™¸í•˜ê³ ëŠ” ììœ ì…ë‹ˆë‹¤.\n",
        "\n",
        "    íŒíŠ¸: collection ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Counter ê°ì²´ë¥¼ í™œìš©í•´ë³´ì„¸ìš”.\n",
        "\n",
        "    Arguments:\n",
        "    sentences -- Vocabularyë¥¼ ë§Œë“¤ê¸° ìœ„í•œ í† í°í™”ëœ ë¬¸ì¥ë“¤\n",
        "    min_freq -- ë‹¨ì¼ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ê¸° ìœ„í•œ ìµœì†Œ ë¹ˆë„\n",
        "                ë°ì´í„°ì…‹ì—ì„œ ìµœì†Œ ë¹ˆë„ë³´ë‹¤ ë” ì ê²Œ ë“±ì¥í•˜ëŠ” í† í°ì€ [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Return:\n",
        "    id2token -- idë¥¼ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” í† í°ì„ ë°˜í™˜í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ \n",
        "    token2id -- í† í°ì„ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” idë¥¼ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    ### ANSWER HERE ###\n",
        "    tokens=[]\n",
        "    for sentence in sentences:\n",
        "      tokens+=sentence\n",
        "    \n",
        "\n",
        "    counter= dict(Counter(tokens))\n",
        "    token2id: Dict[str, int] = {unk_token: unk_token_id}\n",
        "\n",
        "    id2token: List[str] = [unk_token]\n",
        "    for key,values in counter.items():\n",
        "      if values < min_freq:\n",
        "        continue\n",
        "      id2token.append(key)\n",
        "      token2id[key] = id2token.index(key)\n",
        "    \n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    assert id2token[unk_token_id] == unk_token and token2id[unk_token] == unk_token_id, \\\n",
        "        \"[UNK] í† í°ì„ ì ì ˆíˆ ì‚½ì…í•˜ì„¸ìš”\"\n",
        "    assert len(id2token) == len(token2id), \\\n",
        "        \"id2wordê³¼ word2idì˜ í¬ê¸°ëŠ” ê°™ì•„ì•¼ í•©ë‹ˆë‹¤\"\n",
        "    return id2token, token2id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM-682sin8hZ"
      },
      "source": [
        "**ë¬¸ì œ 1-Bì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "rE9cjJWgn8hZ",
        "outputId": "3942971b-9041-4b93-a263-53967b0be9d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Vocabulary Builder Test Cases======\n",
            "ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
            "ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\n",
            "ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\n"
          ]
        }
      ],
      "source": [
        "print (\"======Vocabulary Builder Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentences = [[\"this\", \"sentence\", \"be\", \"tokenized\", \"propery\", \".\"],\n",
        "                [\"jhon\", \"'s\", \"book\", \"is\", \"n't\", \"popular\", \",\", \"but\", \"he\", \"loves\", \"his\", \"book\", \".\"]]\n",
        "\n",
        "id2token, token2id = build_vocab(sentences, min_freq=1)\n",
        "assert sentences == [[id2token[token2id[token]] for token in sentence] for sentence in sentences], \\\n",
        "    \"token2idì™€ id2tokenì´ ì„œë¡œ ì—­ë§¤í•‘ì´ ì•„ë‹™ë‹ˆë‹¤.\"\n",
        "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "# Second test\n",
        "sentences = [[\"a\", \"b\", \"c\", \"d\", \"e\"],\n",
        "                [\"c\", \"d\", \"f\", \"g\"],\n",
        "                [\"d\", \"e\", \"g\", \"h\"]]\n",
        "\n",
        "id2token, token2id = build_vocab(sentences, min_freq=2)\n",
        "assert set(token2id.keys()) == {unk_token, 'c', 'd', 'e', 'g'} == set(id2token) and len(id2token) == 5, \\\n",
        "    \"min_freq ì¸ìê°€ ì œëŒ€ë¡œ ì‘ë™ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
        "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tvfob9_n8ha"
      },
      "source": [
        "### 1-C) ì¸ì½”ë”© ë° ë””ì½”ë”©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimftJ_Wn8ha"
      },
      "source": [
        "ì´ì œ ë¬¸ì¥ì„ ë°›ì•„ í† í°í™”í•˜ê³  ì´ë“¤ì„ ì ì ˆí•œ idë“¤ë¡œ ë°”ê¾¸ëŠ” ì¸ì½”ë”© í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "SkeX30vJn8ha"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def encode(\n",
        "    tokenize: Callable[[str], List[str]],\n",
        "    sentence: str,\n",
        "    token2id: Dict[str, int]\n",
        ") -> List[str]:\n",
        "    \"\"\" ì¸ì½”ë”©\n",
        "    ë¬¸ì¥ì„ ë°›ì•„ í† í°í™”í•˜ê³  ì´ë“¤ì„ ì ì ˆí•œ idë“¤ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "    í† í°í™” ë° ì¸ë±ì‹±ì€ ì¸ìë¡œ ë“¤ì–´ì˜¨ tokenize í•¨ìˆ˜ì™€ ì¸ìë¡œ ì£¼ì–´ì§„ token2idë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
        "    Vocabì— ì—†ëŠ” ë‹¨ì–´ëŠ” [UNK] í† í°ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Arguments:\n",
        "    tokenize -- í† í°í™” í•¨ìˆ˜: ë¬¸ì¥ì„ ë°›ìœ¼ë©´ í† í°ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
        "    token2id -- í† í°ì„ ë°›ìœ¼ë©´ í•´ë‹¹í•˜ëŠ” idë¥¼ ë°˜í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
        "    \n",
        "    Return:\n",
        "    token_ids -- ë¬¸ì¥ì„ ì¸ì½”ë”©í•˜ì—¬ ìˆ«ìë¡œ ë³€í™˜í•œ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    token_ids: List[int] = list()\n",
        "\n",
        "    tokens=tokenize(sentence)\n",
        "    for token in tokens:\n",
        "      if token in token2id:\n",
        "        token_ids.append(token2id[token])\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return token_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2WVOPIln8hb"
      },
      "source": [
        "ê±°ê¾¸ë¡œ idë“¤ì´ ìˆì„ ë•Œ ì›ë¬¸ì¥ì„ ë³µì›í•˜ëŠ” ë””ì½”ë”© í•¨ìˆ˜ë„ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "ê·¸ëŸ¬ë‚˜ í† í°í™” ê³¼ì •ì—ì„œ ê³µë°± ë° ëŒ€ì†Œë¬¸ì ì •ë³´ë¥¼ ìƒì–´ë²„ë¦¬ê³ , [UNK] í† í°ìœ¼ë¡œ ì¸í•´ ì›ë¬¸ì¥ì„ ë³µì›í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "ë•Œë¬¸ì—, ë‹¨ìˆœíˆ ê³µë°±ìœ¼ë¡œ ì—°ê²°ëœ ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©í•©ì‹œë‹¤. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "0Rl1a-v9n8hb"
      },
      "outputs": [],
      "source": [
        "def decode(\n",
        "    token_ids: List[int],\n",
        "    id2token: List[str]\n",
        ") -> str:\n",
        "    \"\"\" ë””ì½”ë”©\n",
        "    ê° idë¥¼ ì ì ˆí•œ í† í°ìœ¼ë¡œ ë°”ê¾¸ê³  ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    return ' '.join(id2token[token_id] for token_id in token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_WWwt1yn8hb"
      },
      "source": [
        "**ì•ì„œ ë§Œë“  í•¨ìˆ˜ë¡œ ë§ë­‰ì¹˜ë¥¼ ì¸ì½”ë”©í•´ë´…ì‹œë‹¤.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "bpN8gyv9n8he"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "id2token, token2id = build_vocab(list(map(tokenize, corpus)), min_freq=2)\n",
        "input_ids = list(map(partial(encode, tokenize, token2id=token2id), corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "f_d1_m2Xn8hf",
        "outputId": "fd467c15-3906-4ac0-d5f7-26607439a7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======1=====\n",
            "ì›ë¬¸: A young man participates in a career while the subject who records it smiles.\n",
            "\n",
            "ì¸ì½”ë”© ê²°ê³¼: [1, 2, 3, 4, 1, 5, 6, 7, 8, 9, 10]\n",
            "ë””ì½”ë”© ê²°ê³¼: a young man in a while the who it . \n",
            "\n",
            "\n",
            "======2=====\n",
            "ì›ë¬¸: The man is scratching the back of his neck while looking for a book in a book store.\n",
            "\n",
            "ì¸ì½”ë”© ê²°ê³¼: [6, 3, 11, 12, 6, 13, 14, 15, 5, 16, 17, 1, 18, 4, 1, 18, 19, 9, 10]\n",
            "ë””ì½”ë”© ê²°ê³¼: the man is scratching the back of his while looking for a book in a book store . \n",
            "\n",
            "\n",
            "======3=====\n",
            "ì›ë¬¸: A person wearing goggles and a hat is sled riding.\n",
            "\n",
            "ì¸ì½”ë”© ê²°ê³¼: [1, 20, 21, 22, 23, 1, 24, 11, 25, 9, 10]\n",
            "ë””ì½”ë”© ê²°ê³¼: a person wearing goggles and a hat is riding . \n",
            "\n",
            "\n",
            "======4=====\n",
            "ì›ë¬¸: A girl in a pink coat and flowered goloshes sledding down a hill.\n",
            "\n",
            "ì¸ì½”ë”© ê²°ê³¼: [1, 26, 4, 1, 27, 28, 23, 29, 30, 1, 31, 9, 10]\n",
            "ë””ì½”ë”© ê²°ê³¼: a girl in a pink coat and flowered down a hill . \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
        "    print(f\"======{sid}=====\")\n",
        "    print(f\"ì›ë¬¸: {sentence}\")\n",
        "    print(f\"ì¸ì½”ë”© ê²°ê³¼: {token_ids}\"),\n",
        "    print(f\"ë””ì½”ë”© ê²°ê³¼: {decode(token_ids, id2token)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2Mq2_on8hf"
      },
      "source": [
        "### 2. [Spacy](https://spacy.io/)ë¥¼ ì´ìš©í•œ ì˜ì–´ í…ìŠ¤íŠ¸ í† í°í™” ë° ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "olkGCTT1n8hf",
        "outputId": "a1e353ba-019d-4913-efb2-1abf3419cad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.0 MB 10.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "! pip install spacy\n",
        "! python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xvivhdbn8hf"
      },
      "source": [
        "### 2-A) Spacyë¥¼ í™œìš©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "CHnlmfnqn8hg"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy_tokenizer = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlKPMMrtn8hg"
      },
      "source": [
        "Spacyë¥¼ í™œìš©í•œ í† í°í™”ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, í† í°í™” ì™¸ì—ë„ í’ˆì‚¬ ë° ë‹¨ì–´ì˜ ê¸°ë³¸í˜• ì •ë³´ ë“± í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•´ ë§ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "2QYwB3Bcn8hg",
        "outputId": "79a89eca-be61-4575-dde1-144f74a325da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Jhon', 'Jhon', 'PROPN'), (\"'s\", \"'s\", 'PART'), ('book', 'book', 'NOUN'), ('is', 'be', 'AUX'), (\"n't\", 'not', 'PART'), ('popular', 'popular', 'ADJ'), (',', ',', 'PUNCT'), ('but', 'but', 'CCONJ'), ('he', '-PRON-', 'PRON'), ('loves', 'love', 'VERB'), ('his', '-PRON-', 'DET'), ('book', 'book', 'NOUN'), ('.', '.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "tokens = spacy_tokenizer(\"Jhon's book isn't popular, but he loves his book.\")\n",
        "print ([(token.text, token.lemma_, token.pos_) for token in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaHwA3Lsn8hg"
      },
      "source": [
        "**ë¶ˆìš©ì–´(Stopword)**\n",
        "\n",
        "ë¶ˆìš©ì–´ë€ í•œ ì–¸ì–´ì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ëœ¯í•©ë‹ˆë‹¤.\n",
        "ê³ ì „ì ì¸ ìì—°ì–´ ì²˜ë¦¬ì—ì„œëŠ” ì´ëŸ¬í•œ ë‹¨ì–´ë“¤ì€ ë¶„ì„ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ìƒê°í•˜ì˜€ê¸° ë•Œë¬¸ì— ì´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
        "`Spacy`ì—ì„œëŠ” ë¶ˆìš©ì–´ ë‹¨ì–´ì˜ ëª©ë¡ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "kY5QqCTBn8hg",
        "outputId": "95bdd020-fe25-484f-ff10-90b98a1d6b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fifty', \"'d\", 'latter', 'twelve', 'up', 'him', 'whoever', 'mine', 'one', 'may', 'sometimes', 'next', 'who', 'former', 'sometime', 'while', 'â€˜m', 'very', 'take', 'put', 'their', 'whom', 'twenty', 'go', 'someone', 'and', 'alone', 'became', 'over', 'was', \"'m\", 'rather', 'they', 'her', 'your', 'â€™m', 'across', 'why', 'nevertheless', 'using', 're', 'formerly', 'top', 'wherever', 'them', 'myself', 'enough', 'us', 'whose', 'hundred', 'without', 'many', 'ours', 'can', 'even', 'themselves', 'thereupon', 'whether', 'noone', 'being', 'fifteen', 'could', 'four', 'under', 'eight', 'has', 'as', 'therein', 'yet', 'last', 'because', 'these', 'â€™ve', 'upon', 'how', 'thus', 'meanwhile', 'nine', 'though', 'on', 'â€˜re', 'yourselves', 'whence', 'therefore', 'neither', 'always', 'else', 'less', \"'re\", 'beforehand', 'say', 'â€™d', 'bottom', 'so', 'its', 'least', 'wherein', 'anything', 'throughout', 'such', 'per', 'each', 'perhaps', 'before', 'also', 'show', 'becoming', 'name', 'â€™s', 'am', 'what', 'within', 'does', 'onto', 'with', 'nâ€™t', 'another', 'both', 'my', 'either', 'the', 'would', 'were', 'made', 'â€˜ll', 'everything', 'or', 'beside', 'down', 'should', 'then', 'sixty', 'hereupon', \"'ll\", 'along', 'from', 'together', 'nobody', 'thereafter', 'now', 'back', 'â€˜ve', 'here', 'other', 'nothing', 'have', 'too', 'used', 'off', 'otherwise', 'some', 'seeming', 'further', 'nowhere', 'all', 'a', 'anyone', 'whither', 'only', 'nor', 'others', 'thence', 'after', 'his', 'still', 'really', 'itself', 'that', 'never', 'yourself', 'whole', 'every', 'herein', 'whereafter', 'more', 'doing', 'often', 'everywhere', 'of', 'hers', 'below', 'above', 'me', 'done', 'than', 'toward', 'give', 'somehow', 'besides', 'i', 'not', 'but', 'where', 'moreover', 'two', 'unless', 'most', 'seems', 'five', 'something', 'almost', 'mostly', 'already', 'anyhow', 'regarding', 'until', 'serious', 'might', 'quite', 'â€™ll', 'cannot', 'through', 'third', 'whereby', 'towards', 'eleven', 'same', 'between', 'hence', 'no', 'first', 'if', 'much', 'did', 'â€™re', 'seemed', 'at', 'whenever', 'somewhere', 'none', 'be', 'when', 'elsewhere', 'due', 'again', 'well', 'via', 'please', 'forty', 'this', 'although', 'three', 'except', 'own', 'himself', 'nâ€˜t', 'move', 'will', 'among', 'are', 'just', 'into', 'against', 'had', 'part', 'is', 'full', 'afterwards', 'keep', 'indeed', 'anyway', 'once', 'there', \"'ve\", 'becomes', 'she', 'get', 'however', 'since', 'in', 'we', 'he', 'been', 'beyond', 'amongst', 'ourselves', 'whereupon', 'anywhere', 'front', 'ten', 'various', 'several', 'thereby', 'amount', 'â€˜d', 'ca', \"n't\", 'an', 'seem', 'everyone', 'hereafter', 'side', 'for', 'whereas', 'those', 'hereby', 'yours', 'about', 'by', 'â€˜s', 'you', 'it', 'latterly', 'out', 'must', 'herself', 'ever', 'our', 'around', 'any', 'see', 'become', 'make', 'whatever', 'six', 'few', 'thru', 'do', 'call', 'behind', 'to', 'during', 'which', \"'s\", 'empty', 'namely'}\n"
          ]
        }
      ],
      "source": [
        "print(spacy.lang.en.stop_words.STOP_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeAi-qnrn8hh"
      },
      "source": [
        "### 2-B) Spacyë¥¼ í™œìš©í•œ ì „ì²˜ë¦¬ ë° í† í°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "aK4APB7xn8hh"
      },
      "outputs": [],
      "source": [
        "def spacy_tokenize(\n",
        "    tokenizer: spacy.language.Language,\n",
        "    sentence: str\n",
        ") -> List[str]:\n",
        "    \"\"\" Spacyë¥¼ í™œìš©í•œ í† í¬ë‚˜ì´ì € êµ¬í˜„\n",
        "    Spacyë¥¼ í™œìš©í•´ì„œ í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì´ë•Œ ë¶ˆìš©ì–´ëŠ” ì œì™¸í•˜ê³  ì–´ê°„ì„ í† í°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    \n",
        "    ì˜ˆì‹œ: 'I don't like Jenifer's work.'\n",
        "    ==> ['I', 'like', 'Jenifer', 'work', '.']\n",
        "\n",
        "    Arguments:\n",
        "    tokenizer -- Spacy í† í°í™”ê¸°\n",
        "    sentence -- í† í°í™”í•  ì˜ë¬¸ ë¬¸ì¥\n",
        "    \n",
        "    Return:\n",
        "    tokens -- ë¶ˆìš©ì–´ ì œê±° ë° í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    \n",
        "    ### YOUR CODE HERE \n",
        "    ### ANSWER HERE ###\n",
        "    tokens: List[str] = list()\n",
        "    sentence=sentence.lower()\n",
        "    tokenized = tokenizer(sentence)\n",
        "\n",
        "    for token in tokenized:\n",
        "      if token.text not in spacy.lang.en.stop_words.STOP_WORDS:\n",
        "        tokens.append(token.text)\n",
        "    print(tokens)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0HoPbfAn8hh"
      },
      "source": [
        "**ë¬¸ì œ 2-Bì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì½”ë“œ**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "Yrimnj3qn8hh",
        "outputId": "f94d2d8c-978a-4922-811d-f150022832fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Spacy Tokenizer Test Cases======\n",
            "['jhon', 'book', 'popular', ',', 'loves', 'book', '.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-200-eab03c85cef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tokenize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'properly'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;34m\"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤."
          ]
        }
      ],
      "source": [
        "print (\"======Spacy Tokenizer Test Cases======\")\n",
        "\n",
        "# First test\n",
        "sentence = \"This sentence should be tokenized properly.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
        "assert tokens == ['this', 'sentence', 'tokenize', 'properly', '.'], \\\n",
        "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
        "print(\"ì²«ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "# Second test\n",
        "sentence = \"Jhon's book isn't popular, but he loves his book.\"\n",
        "tokens = spacy_tokenize(spacy_tokenizer, sentence)\n",
        "assert tokens == ['Jhon', 'book', 'popular', ',', 'love', 'book', '.'], \\\n",
        "    \"í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ê°€ ê¸°ëŒ€ ê²°ê³¼ì™€ ë‹¤ë¦…ë‹ˆë‹¤.\"\n",
        "print(\"ë‘ë²ˆì§¸ í…ŒìŠ¤íŠ¸ í†µê³¼!\")\n",
        "\n",
        "print(\"ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2EB-FwGn8hh"
      },
      "source": [
        "**ì•ì„œ ë§Œë“  í•¨ìˆ˜ë¡œ ë§ë­‰ì¹˜ë¥¼ ì¸ì½”ë”©í•´ë´…ì‹œë‹¤.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAlMF-i_n8hi"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "my_tokenize = partial(spacy_tokenize, spacy_tokenizer)\n",
        "id2token, token2id = build_vocab(list(map(my_tokenize, tqdm(corpus, desc=\"Building\"))), min_freq=3)\n",
        "input_ids = list(map(partial(encode, my_tokenize, token2id=token2id), tqdm(corpus, desc=\"Tokenizing\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUrSVg0-n8hi"
      },
      "outputs": [],
      "source": [
        "for sid, sentence, token_ids in zip(range(1, 5), corpus, input_ids):\n",
        "    print(f\"======{sid}=====\")\n",
        "    print(f\"ì›ë¬¸: {sentence}\")\n",
        "    print(f\"ì¸ì½”ë”© ê²°ê³¼: {token_ids}\"),\n",
        "    print(f\"ë””ì½”ë”© ê²°ê³¼: {decode(token_ids, id2token)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvPOazVbn8hj"
      },
      "source": [
        "### 3. [Konlpy](https://konlpy.org/ko/latest/)ë¥¼ í™œìš©í•œ í•œêµ­ì–´ í† í°í™”\n",
        "í•œêµ­ì–´ì—ì„œ \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ëŠ”ë‹¤\"ë¼ëŠ” ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ['ë‚˜', 'ëŠ”', 'ë°¥', 'ì„', 'ë¨¹ëŠ”ë‹¤']\n",
        "\n",
        "í•œêµ­ì–´ì—ì„œ \"ë‹¨ì–´\"ëŠ” ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŠ” í•œêµ­ì–´ê°€ ê°–ê³  ìˆëŠ” \"êµì°©ì–´\"ë¡œì„œì˜ íŠ¹ì§• ë•Œë¬¸ì…ë‹ˆë‹¤. \n",
        "ì²´ì–¸ ë’¤ì— ì¡°ì‚¬ê°€ ë¶™ëŠ” ê²ƒì´ ëŒ€í‘œì ì¸ íŠ¹ì§•ì´ë©° ì˜ë¯¸ ë‹¨ìœ„ê°€ êµ¬ë¶„ë˜ê³  ìë¦½ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ì¡°ì‚¬ëŠ” \"ë‹¨ì–´\"ì…ë‹ˆë‹¤.\n",
        "\n",
        "í•œêµ­ì–´ì—ì„œëŠ” ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” ë°©ë²•ëŠ” ê³µë°±ì— ê¸°ë°˜í•˜ì§€ ì•Šê³  ì‚¬ìš©í•˜ì§€ ì•Šê³  í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "(ì°¸ê³  1: [êµ­ë¦½ êµ­ì–´ì›: \"ì¡°ì‚¬ëŠ” ë‹¨ì–´ì´ë‹¤\"](https://www.korean.go.kr/front/onlineQna/onlineQnaView.do?mn_id=216&qna_seq=26915#:~:text='%EC%A1%B0%EC%82%AC'%EB%8A%94%20%EC%99%84%EC%A0%84%ED%95%9C%20%EC%9E%90%EB%A6%BD%EC%84%B1%EC%9D%80,%ED%95%98%EC%97%AC%20%EB%8B%A8%EC%96%B4%EB%A1%9C%20%EC%B2%98%EB%A6%AC%ED%95%A9%EB%8B%88%EB%8B%A4.) )\n",
        "\n",
        "(ì°¸ê³  2: [Konlpy: í˜•íƒœì†Œ ë¶„ì„ê¸°](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Qc2By1n8hj"
      },
      "outputs": [],
      "source": [
        "! apt-get install -y build-essential openjdk-8-jdk python3-dev curl git automake\n",
        "! pip install konlpy \"tweepy<4.0.0\"\n",
        "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQjXTspTn8hj"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Mecab\n",
        "tokenizer = Mecab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yNUkJRWn8hk"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\\\n",
        "ìœ êµ¬í•œ ì—­ì‚¬ì™€ ì „í†µì— ë¹›ë‚˜ëŠ” ìš°ë¦¬ ëŒ€í•œêµ­ë¯¼ì€ \\\n",
        "3Â·1ìš´ë™ìœ¼ë¡œ ê±´ë¦½ëœ ëŒ€í•œë¯¼êµ­ì„ì‹œì •ë¶€ì˜ ë²•í†µê³¼ ë¶ˆì˜ì— í•­ê±°í•œ 4Â·19ë¯¼ì£¼ì´ë…ì„ ê³„ìŠ¹í•˜ê³ , \\\n",
        "ì¡°êµ­ì˜ ë¯¼ì£¼ê°œí˜ê³¼ í‰í™”ì  í†µì¼ì˜ ì‚¬ëª…ì— ì…ê°í•˜ì—¬ ì •ì˜Â·ì¸ë„ì™€ ë™í¬ì• ë¡œì¨ ë¯¼ì¡±ì˜ ë‹¨ê²°ì„ ê³µê³ íˆ í•˜ê³ , \\\n",
        "ëª¨ë“  ì‚¬íšŒì  íìŠµê³¼ ë¶ˆì˜ë¥¼ íƒ€íŒŒí•˜ë©°, \\\n",
        "ììœ¨ê³¼ ì¡°í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ììœ ë¯¼ì£¼ì  ê¸°ë³¸ì§ˆì„œë¥¼ ë”ìš± í™•ê³ íˆ í•˜ì—¬ \\\n",
        "ì •ì¹˜Â·ê²½ì œÂ·ì‚¬íšŒÂ·ë¬¸í™”ì˜ ëª¨ë“  ì˜ì—­ì— ìˆì–´ì„œ ê°ì¸ì˜ ê¸°íšŒë¥¼ ê· ë“±íˆ í•˜ê³ , \\\n",
        "ëŠ¥ë ¥ì„ ìµœê³ ë„ë¡œ ë°œíœ˜í•˜ê²Œ í•˜ë©°, ììœ ì™€ ê¶Œë¦¬ì— ë”°ë¥´ëŠ” ì±…ì„ê³¼ ì˜ë¬´ë¥¼ ì™„ìˆ˜í•˜ê²Œ í•˜ì—¬, \\\n",
        "ì•ˆìœ¼ë¡œëŠ” êµ­ë¯¼ìƒí™œì˜ ê· ë“±í•œ í–¥ìƒì„ ê¸°í•˜ê³  ë°–ìœ¼ë¡œëŠ” í•­êµ¬ì ì¸ ì„¸ê³„í‰í™”ì™€ ì¸ë¥˜ê³µì˜ì— ì´ë°”ì§€í•¨ìœ¼ë¡œì¨ \\\n",
        "ìš°ë¦¬ë“¤ê³¼ ìš°ë¦¬ë“¤ì˜ ìì†ì˜ ì•ˆì „ê³¼ ììœ ì™€ í–‰ë³µì„ ì˜ì›íˆ í™•ë³´í•  ê²ƒì„ ë‹¤ì§í•˜ë©´ì„œ \\\n",
        "1948ë…„ 7ì›” 12ì¼ì— ì œì •ë˜ê³  8ì°¨ì— ê±¸ì³ ê°œì •ëœ í—Œë²•ì„ ì´ì œ êµ­íšŒì˜ ì˜ê²°ì„ ê±°ì³ êµ­ë¯¼íˆ¬í‘œì— ì˜í•˜ì—¬ ê°œì •í•œë‹¤.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPf54Fuen8hk"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.pos(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mhdo9Ktn8hk"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.morphs(text))"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "d56be8d41f6ed650baad816e622386c6d957316b9dcbfdf7cffae878b7a3098b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('default')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "[Basic 1] Data Preprocessing & Tokenization.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}