# -*- coding: utf-8 -*-
"""1. Affine Function.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k_lG7XUmee4J3Q2nCvdw4T89IsHNCGG5

# 1-1 Affine Function

##Code 1-1-1 : Affine function
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense

x= tf.constant([[10.,20.]]) # input setting (Note : input > matrix)
print(x)
dense = Dense(units=1, activation='linear') #imp. an affine function

y_tf = dense(x) #fowrad propagation + params initialization
print(y_tf)
W,B = dense.get_weights() # get weight, bias

print(W,B)

y_man = tf.linalg.matmul(x,W) + B

#print results
print('========== INPUT/weight/biase =========')
print('x: {}\n{}\n' . format(x.shape, x.numpy()))
print('w: {}\n{}\n' . format(W.shape, W))
print('b: {}\n{}\n' . format(B.shape, B))

print('========== OUTPUT =========')
print('y(Tensorflow) : {} \n{}\n' .format(y_tf.shape,y_tf.numpy()))
print('y(Manual) : {} \n{}\n' .format(y_man.shape,y_man.numpy()))

"""## Code 1-1-2 : Params initialization"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.initializers import Constant

x= tf.constant([[20.],[10.]]) # input setting (Note : input > matrix)

#weight/bias setting
w,b = tf.constant(10.) , tf.constant(20.)
w_init , b_init  = Constant(w), Constant(b)

print(w_init,b_init)

dense =Dense(units = 1, 
             activation='linear', 
             kernel_initializer=w_init, 
             bias_initializer = b_init)
y_tf = dense(x)
print(y_tf)
W,B = dense.get_weights()

#print results
print("W: {}\n{}\n" . format(W.shape, W))
print("B: {}\n{}\n" . format(B.shape, B))



"""#1-2 Affine Functions with n Features

##Code 1-2-1 Affine Functions with n Features
"""

import tensorflow as tf

from tensorflow.keras.layers import Dense

x= tf.random.uniform(shape=(1,10) , minval = 0 ,maxval = 10) # (x,y,z 등 다른 인풋값들)들어가야하는 값들 : row vector
                                                             # 벡터?? 스칼라???
print(x.shape, '\n' , x)

dense = Dense(units =1)

y_tf = dense(x) #input이 10개 종류기때문에 weight도 10개로 만들어줌 (column vetor로 나옴)

W, B = dense.get_weights() 

y_man = tf.linalg.matmul(x,W) + B # linear function

#print results
print('========== INPUT/weight/biase =========')
print('x: {}\n{}\n' . format(x.shape, x.numpy()))
print('w: {}\n{}\n' . format(W.shape, W))
print('b: {}\n{}\n' . format(B.shape, B))

print('========== OUTPUT =========')
print('y(Tensorflow) : {} \n{}\n' .format(y_tf.shape,y_tf.numpy()))
print('y(Manual) : {} \n{}\n' .format(y_man.shape,y_man.numpy()))

"""# 1-3 Activation Function

## Code 1-3-1 Activation layer
"""

import tensorflow as tf

from tensorflow.math import exp, maximum
from tensorflow.keras.layers import Activation

x= tf.random.normal(shape=(1,5)) # input setting

#imp, activation function
sigmoid = Activation('sigmoid')
tanh = Activation('tanh')
relu= Activation('relu')

#foward propagtion
y_sigmoid_tf = sigmoid(x)
y_tanh_tf = tanh(x)
y_relu_tf = relu(x)

#forward propagation(manual)
y_sigmoid_man = 1 / (1+exp(-x))
y_tanh_man = (exp(x)- exp(-x))/(exp(x)+ exp(-x))
y_relu_man = maximum(x,0)


print("x:{}\n{}" . format(x.shape,x.numpy()))


print("Sigmoid(Tensorflow) : {} \n{}" .format(y_sigmoid_tf.shape,y_sigmoid_tf.numpy()))
print("Sigmoid(Manual) : {} \n{}" .format(y_sigmoid_man.shape,y_sigmoid_man.numpy()))

print("Tanh(Tensorflow) : {} \n{}" .format(y_tanh_tf.shape,y_tanh_tf.numpy()))
print("Tanh(Manual) : {} \n{}" .format(y_tanh_man.shape,y_tanh_man.numpy()))

print("Relu(Tensorflow) : {} \n{}" .format(y_relu_tf.shape,y_relu_tf.numpy()))
print("Relu(Manual) : {} \n{}" .format(y_relu_man.shape,y_relu_man.numpy()))



"""## Code 1-3-2 Activation in Dense layer

"""

import tensorflow as tf

from tensorflow.keras.layers import Dense
from tensorflow.math import exp, maximum


x= tf.random.normal(shape=(1,5)) #input settings

#imp. artificial neurons
dense_sigmoid = Dense(units=1 , activation='sigmoid')
dense_tanh = Dense(units=1 , activation='tanh')
dense_relu = Dense(units=1 , activation='relu')

#forward propagation
y_sigmoid = dense_sigmoid(x)
y_tanh = dense_tanh(x)
y_relu = dense_relu(x)

print("AN with Sigmoid : {}\n {}" . format(y_sigmoid.shape, y_sigmoid.numpy()))
print("AN with Tanh : {}\n {}" . format(y_tanh.shape, y_tanh.numpy()))
print("AN with Relu : {}\n {}" . format(y_relu.shape, y_relu.numpy()))


#forward propagation(manual)
print('\n====\n')

W,b = dense_sigmoid.get_weights()
z = tf.linalg.matmul(x,W) + B
a = 1 /(1+exp(-z))
print("Activation value(manual) : {}\n{}" . format(a.shape,a.numpy()))
print("AN with Sigmoid : {}\n {}" . format(y_sigmoid.shape, y_sigmoid.numpy()))



"""# 1-4 Artificial Neurons

## Code 1-4-1 Artificial Neurons
"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.math import exp,maximum

activation = "sigmoid"
#activation = 'tanh'
#activation = 'relu'

x= tf.random.uniform(shape=(1,10)) #input settings

dense = Dense(units=1 , activation = activation) # imp, an affine + activation
y_tf = dense(x)
W, B = dense.get_weights()

y_man = tf.linalg.matmul(x,W) + B
if activation == "sigmoid" :
  y_man = 1/(1+exp(-y_man))
elif activation == "tanh" :
  y_man = (exp(y_man)-exp(-y_man)) / (exp(y_man)+exp(-y_man))
elif activation == 'relu':
    y_man = maximum(0,y_man)

print("Activiation : " , activation)
print("y_tf : {}\n{}" . format(y_tf.shape,y_tf.numpy()))
print("y_man : {}\n{}" . format(y_man.shape,y_man.numpy()))



"""# 1-5 Minibatches"""



"""## Code 1-5-1 : Shapes of Dense layers"""

import tensorflow as tf

from tensorflow.keras.layers import Dense

N,n_feature = 8, 10 #set input params
                    # N : 차원을유지하면서 들어가는 변수갯수들( 몇명??)
                    # n_feature : x,y,z 들 각각 다른변수( 몸무게, 키 ,나이)
x= tf.random.normal(shape=(N,n_feature)) # generate minibatch


dense = Dense(units=1 , activation= "relu") # imp, an AN
y = dense(x) # forward propagation


W, B = dense.get_weights() # get weight, bias

#print results

print("Shape of x : {}" , x.shape)
print("Shape of W : {}" , W.shape)
print("Shape of B : {}" , B.shape)

"""## Code 1-5-2 : Output Calculations"""

import tensorflow as tf

from tensorflow.keras.layers import Dense

N,n_feature = 8, 10 #set input params
                    # N : 차원을유지하면서 들어가는 변수갯수들( 몇명??)
                    # n_feature : x,y,z 들 각각 다른변수( 몸무게, 키 ,나이)
x= tf.random.normal(shape=(N,n_feature)) # generate minibatch


dense = Dense(units=1 , activation= "sigmoid") # imp, an AN

y_tf = dense(x) # forward propagation (Tensorflow)

W, B = dense.get_weights() # get weight, bias

y_man=tf.linalg.matmul(x,W)+ B # forward propagation (Manual)
y_man = 1/(1+tf.math.exp(-y_man))
#print results
print("output(Tensorflow) : \n" , y_tf.numpy())
print("output(Manual) : \n" , y_man.numpy())

