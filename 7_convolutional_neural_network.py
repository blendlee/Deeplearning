# -*- coding: utf-8 -*-
"""7. Convolutional_Neural Network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCBiluNXPJHtKiRr3L0k9AKWtAitfpAs
"""



"""# 7-1 Shapes in CNNs

## Code 7-1-1 Shapes in the Feature Extractors
"""

import tensorflow as tf

from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten

batch_size =32
N, n_H,n_W,n_C = 32,28,28,3
n_conv_filter =5
k_size=3
pool_size, pool_strides=2,2


x=tf.random.normal(shape=(N, n_H,n_W,n_C))

conv1=Conv2D(filters=n_conv_filter, kernel_size=k_size, padding='same', activation='relu')

conv1_pool= MaxPooling2D(pool_size=pool_size , strides=pool_strides)


conv2 =Conv2D(filters=n_conv_filter, kernel_size=k_size, padding='same', activation='relu')

conv2_pool= MaxPooling2D(pool_size=pool_size , strides=pool_strides)


flatten= Flatten()
print("Input : {} \n" .format(x.shape))

x = conv1(x)
print("After conv1 : {} \n" .format(x.shape))


x= conv1_pool(x)
print("After conv1 pool : {} \n" .format(x.shape))

x=conv2(x)
print("After conv2 : {} \n" .format(x.shape))

x=conv2_pool(x)
print("After conv2 pool : {} \n" .format(x.shape))

x= flatten(x)
print("After flatten : {} \n" .format(x.shape))

"""##Code 7-1-2 Shapes in the Classifier"""

import tensorflow as tf

from tensorflow.keras.layers import Dense

n_neurons= [50,25,10]


dense1 = Dense(units=n_neurons[0], activation='relu')
dense2 = Dense(units=n_neurons[1], activation='relu')
dense3 = Dense(units=n_neurons[2], activation='softmax')

print("Input feature: {}" .format(x.shape))
x= dense1(x)
W,B = dense1.get_weights()
print("W/B {}/{}" .format(W.shape, B.shape))
print('After dense1: {}\n' .format(x.shape))


x= dense2(x)
W,B = dense2.get_weights()
print("W/B {}/{}" .format(W.shape, B.shape))
print('After dense2: {}\n' .format(x.shape))


x= dense3(x)
W,B = dense3.get_weights()
print("W/B {}/{}" .format(W.shape, B.shape))
print('After dense3: {}' .format(x.shape))

"""##Code 7-1-3 Shapes in the Loss Fuctions"""

from tensorflow.keras.losses import CategoricalCrossentropy

y=tf.random.uniform(minval=0, maxval=10, shape=(32,) , dtype=tf.int32)

y =tf.one_hot(y, depth=10)

loss_object = CategoricalCrossentropy()
loss = loss_object(y,x)
print(loss.shape)
print(loss)

"""# 7-2 Implementation of CNNs

## Code 7-2-1 Implementation with Sequential Method
"""

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

N,n_H, n_W, n_C = 4,28,28,3
n_conv_neurons = [10,20,30]
n_dense_neurons =[50,30,10]
k_size, padding = 3, 'same'
pool_size, pool_strides = 2,2
activation='relu'


x=tf.random.normal(shape=(N,n_H, n_W, n_C))

model = Sequential()
model.add(Conv2D(filters=n_conv_neurons[0], kernel_size=k_size, padding=padding, activation='relu'))
model.add(MaxPooling2D(pool_size=pool_size, strides=pool_strides))


model.add(Conv2D(filters=n_conv_neurons[1], kernel_size=k_size, padding=padding, activation='relu'))
model.add(MaxPooling2D(pool_size=pool_size, strides=pool_strides))


model.add(Conv2D(filters=n_conv_neurons[2], kernel_size=k_size, padding=padding, activation='relu'))
model.add(MaxPooling2D(pool_size=pool_size, strides=pool_strides))


model.add(Flatten())
model.add(Dense(units=n_dense_neurons[0], activation=activation))
model.add(Dense(units=n_dense_neurons[1], activation=activation))
model.add(Dense(units=n_dense_neurons[2], activation='softmax'))

predictions=model(x)
print(predictions.shape)

"""## Code 7-2-2 Implementation with Model Sub-Classing"""

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense




class TestCNN(Model):
  def __init__(self):
    super(TestCNN,self).__init__()

    self.conv1 = Conv2D(filters=n_conv_neurons[0], kernel_size=k_size, padding=padding, activation='relu')
    self.conv1_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

    self.conv2 = Conv2D(filters=n_conv_neurons[1], kernel_size=k_size, padding=padding, activation='relu')
    self.conv2_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

    self.conv3 = Conv2D(filters=n_conv_neurons[2], kernel_size=k_size, padding=padding, activation='relu')
    self.conv3_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

    self.flatten=Flatten()

    self.dense1 = Dense(units=n_dense_neurons[0], activation=activation)
    self.dense2 = Dense(units=n_dense_neurons[1], activation=activation)
    self.dense3 = Dense(units=n_dense_neurons[2], activation='softmax')

  def call(self,x):
    print(x.shape)
    x= self.conv1(x)
    print(x.shape)
    x= self.conv1_pool(x)
    print(x.shape)

    x= self.conv2(x)
    print(x.shape)
    x= self.conv2_pool(x)
    print(x.shape)

    x= self.conv3(x)
    print(x.shape)
    x= self.conv3_pool(x)
    print(x.shape)

    x= self.flatten(x)
    print(x.shape)


    x= self.dense1(x)
    print(x.shape)
    x= self.dense2(x)
    print(x.shape)
    x= self.dense3(x)
    print(x.shape)

    return x

N,n_H, n_W, n_C = 4,28,28,3
n_conv_neurons = [10,20,30]
n_dense_neurons =[50,30,10]
k_size, padding = 3, 'same'
pool_size, pool_strides = 2,2
activation='relu'


x=tf.random.normal(shape=(N,n_H, n_W, n_C))


model=TestCNN()

y=model(x)

"""##Code 7-2-3 Implementation wtih Sequential + Layer Sub-Classing"""

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer


from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense


class MyConv(Layer):
  def __init__(self, n_neuron):
    super(MyConv , self).__init__()
      
    self.conv = Conv2D(filters=n_neuron, kernel_size=k_size, padding=padding, activation='relu')
    self.conv_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

  def call(self,x):
    x = self.conv(x)
    x = self.conv_pool(x)
    return x

model = Sequential()
model.add(MyConv(n_conv_neuron[0]))
model.add(MyConv(n_conv_neuron[1]))
model.add(MyConv(n_conv_neuron[2]))

model.add(Flatten())
model.add(Dense(units=n_dense_neurons[0], activation=activation))
model.add(Dense(units=n_dense_neurons[1], activation=activation))
model.add(Dense(units=n_dense_neurons[2], activation='softmax'))

"""##Code 7-2-4 Implementation with Model and Layer Sub-Classing"""

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer


from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense


class MyConv(Layer):
  def __init__(self, n_neuron):
    super(MyConv , self).__init__()
      
    self.conv = Conv2D(filters=n_neuron, kernel_size=k_size, padding=padding, activation='relu')
    self.conv_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

  def call(self,x):
    x = self.conv(x)
    x = self.conv_pool(x)
    return x

class TestCNN(Model):
  def __init__(self):
    super(TestCNN,self).__init__()


    self.conv1 = MyConv(n_conv_neurons[0])
    self.conv1 = MyConv(n_conv_neurons[1])
    self.conv1 = MyConv(n_conv_neurons[2])
    self.flatten=Flatten()

    self.dense1 = Dense(units=n_dense_neurons[0], activation=activation)
    self.dense2 = Dense(units=n_dense_neurons[1], activation=activation)
    self.dense3 = Dense(units=n_dense_neurons[2], activation='softmax')

  def call(self,x):
    x= self.conv1(x)
    x= self.conv2(x)
    x= self.conv3(x)
    x= self.flatten(x)

    x= self.dense1(x)
    x= self.dense2(x)
    x= self.dense3(x)


    return x

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

class MyConv(Layer):
  def __init__(self, n_neuron):
    super(MyConv , self).__init__()
      
    self.conv = Conv2D(filters=n_neuron, kernel_size=k_size, padding=padding, activation='relu')
    self.conv_pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides)

  def call(self,x):
    x = self.conv(x)
    x = self.conv_pool(x)
    return x

class TestCNN(Model):
  def __init__(self):
    super(TestCNN,self).__init__()
    self.fe = Sequential()

    self.fe.add(MyConv(n_conv_neurons[0]))
    self.fe.add(MyConv(n_conv_neurons[1]))
    self.fe.add(MyConv(n_conv_neurons[2]))
    self.fe.add(Flatten())


    self.classifier = Sequential()
    self.classifier.add(Dense(units=n_dense_neurons[0], activation=activation))
    self.classifier.add(Dense(units=n_dense_neurons[1], activation=activation))
    self.classifier.add(Dense(units=n_dense_neurons[2], activation='softmax'))

  def call(self,x):
    x= self.fe(x)
    x= self.classifier(x)


    return x

"""#7-3 Implementation of LeNet

## Code 7-3-1 LeNet with Model-Subclassing
"""

import tensorflow as tf

from tensorflow.keras.models import Model

from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense



class LeNet(Model):
  def __init__(self):
    super(LeNet, self).__init__()

    self.conv1 = Conv2D(filters=6, kernel_size=5, padding='same', activation='tanh')
    self.conv1_pool = AveragePooling2D(pool_size=2, strides=2)

    self.conv2 = Conv2D(filters=6, kernel_size=5, padding='valid', activation='tanh')
    self.conv2_pool = AveragePooling2D(pool_size=2, strides=2)

    self.conv3 = Conv2D(filters=120, kernel_size=5, padding='valid', activation='tanh')
    self.flatten = Flatten()



    self.dense1 = Dense(units=84, activation='tanh')
    self.dense2 = Dense(units=10, activation='softmax')


  def call(self, x):
    print("x : {} \n" .format(x.shape))

    x= self.conv1(x)
    print("x : {} \n" .format(x.shape))
    x= self.conv1_pool(x)
    print("x : {} \n" .format(x.shape))

    x= self.conv2(x)
    print("x : {} \n" .format(x.shape))
    x= self.conv2_pool(x)
    print("x : {} \n" .format(x.shape))

    x= self.conv3(x)
    print("x : {} \n" .format(x.shape))
    x= self.flatten(x)
    print("x : {} \n" .format(x.shape))


    x=self.dense1(x)
    print("x : {} \n" .format(x.shape))
    x=self.dense2(x)
    print("x : {} \n" .format(x.shape))

    return x



model =  LeNet()

x= tf.random.normal(shape=(32,38,28,1))

predictions=model(x)

"""## Code 7-3-2 LeNet with Hybrid method"""

import tensorflow as tf


from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer


from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

class ConvLayer(Layer):
  def __init__(self,filters,padding,pool=True):
    super(ConvLayer, self).__init__()

    self.conv = Conv2D(filters=6, kernel_size=5, padding='same', activation='tanh')
    if pool == True :
      self.conv_pool = AveragePooling2D(pool_size=2, strides=2)

  def call(self,x):
    x= self.conv(x)
    if self.pool == True
      x= self.conv_pool(x)

    return x


class LeNet(Model):
  def __init__(self):
    super(LeNet, self).__init__()
    self.conv1 = ConvLayer(filters=6, padding='same')
    self.conv1 = ConvLayer(filters=16, padding='valid')
    self.conv1 = ConvLayer(filters=120, padding='valid', pool= False)
    self.flatten = Flatten()



    self.dense1 = Dense(units=84, activation='tanh')
    self.dense2 = Dense(units=10, activation='softmax')


  def call(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)

    x = self.flatten(x)
    x=  self.dense1(x)
    x=  self.dense2(x)
    return x



model =  LeNet()

x= tf.random.normal(shape=(32,38,28,1))

predictions=model(x)

"""##Code 7-3-3 Forward Propagation of LeNet"""

import tensorflow as tf
import numpy as np

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer


from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense

from tensorflow.keras.datasets import mnist
from tensorflow.keras.losses import SparseCategoricalCrossentropy


### Lenet implementation ###
class ConvLayer(Layer):
  def __init__(self,filters,padding,pool=True):
    super(ConvLayer, self).__init__()

    self.conv = Conv2D(filters=6, kernel_size=5, padding='same', activation='tanh')
    if pool == True :
      self.conv_pool = AveragePooling2D(pool_size=2, strides=2)

  def call(self,x):
    x= self.conv(x)
    if self.pool == True:
      x= self.conv_pool(x)

    return x


class LeNet(Model):
  def __init__(self):
    super(LeNet, self).__init__()
    self.conv1 = ConvLayer(filters=6, padding='same')
    self.conv2 = ConvLayer(filters=16, padding='valid')
    self.conv3 = ConvLayer(filters=120, padding='valid', pool= False)
    self.flatten = Flatten()



    self.dense1 = Dense(units=84, activation='tanh')
    self.dense2 = Dense(units=10, activation='softmax')


  def call(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)

    x = self.flatten(x)
    x=  self.dense1(x)
    x=  self.dense2(x)
    return x


#### Dataset Preparation ###
(train_images, train_labels), _ = mnist.load_data()
train_images = np.expand_dims(train_images, axis=3).astype(np.float32)
train_labels = train_labels.astype(np.int32)

train_ds = tf.data.Dataset.from_tensor_slices((train_images,train_labels))
train_ds = train_ds.batch(32)


### Foward Propagation ###

model = LeNet()
loss_object = SparseCategoricalCrossentropy()


for images,labels in train_ds:
  predictions = model(images)
  loss = loss_object(labels,predictions)
  print(loss)

